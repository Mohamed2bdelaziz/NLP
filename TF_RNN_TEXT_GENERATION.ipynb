{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mohamed2bdelaziz/NLP/blob/main/TF_RNN_TEXT_GENERATION.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ycmaHWrMtxoS"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install wikipedia"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Imports"
      ],
      "metadata": {
        "id": "RgKIG09kZYsq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wikipedia as wiki\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "from nltk import RegexpTokenizer\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Activation, Embedding\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wwNF-Rtit9Yk"
      },
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download the pretrained glove models embeddings\n",
        "!wget https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ],
      "metadata": {
        "id": "dKtzaxOXIm-p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2569102-07cc-498d-adaa-0212f43d592f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-10 13:14:44--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.02MB/s    in 2m 39s  \n",
            "\n",
            "2024-05-10 13:17:24 (5.18 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Creating text data from Wikipedia api"
      ],
      "metadata": {
        "id": "W9BtaObgcXCj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_these_topics(\n",
        "    topics : list,\n",
        "    language : str = 'en'\n",
        "    ) -> list:\n",
        "  wiki.set_lang(language)\n",
        "  texts = list()\n",
        "  for topic in topics:\n",
        "    try:\n",
        "      topic_page = wiki.page(topic)\n",
        "      texts.append(topic_page.content)\n",
        "    except Exception as e:\n",
        "      print(f\"No page of '{topic}' topic was found\")\n",
        "  return texts"
      ],
      "metadata": {
        "id": "wTy_QhJ-aaS1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topics = [\n",
        "    \"Egypt\",\n",
        "    \"History of modern Egypt\",\n",
        "    \"Mamluk Sultanate\",\n",
        "    \"Fatimid Caliphate\",\n",
        "    \"Arab conquest of Egypt\",\n",
        "    \"Ancient Egypt\"\n",
        "]\n",
        "\n",
        "docs = get_these_topics(topics)"
      ],
      "metadata": {
        "id": "0HgwgqA7uDzX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Our DOCs words lenght are:\\n\")\n",
        "{topic: len(doc.split()) for topic, doc in zip(topics, docs)}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzTGUsNNYvzK",
        "outputId": "eae37be6-c589-4963-8925-a32db008058f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our DOCs words lenght are:\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Egypt': 14089,\n",
              " 'History of modern Egypt': 5005,\n",
              " 'Mamluk Sultanate': 16697,\n",
              " 'Fatimid Caliphate': 12972,\n",
              " 'Arab conquest of Egypt': 5289,\n",
              " 'Ancient Egypt': 12314}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Preprocessing"
      ],
      "metadata": {
        "id": "obsDM2d4hoJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(\n",
        "    document : str\n",
        "    ) -> str:\n",
        "  document = re.sub(r'\\s+', ' ', document, flags=re.I) # Remove extra white space from text\n",
        "  document = re.sub(r'\\W', ' ', str(document)) # Remove all the special characters from text\n",
        "  # document = re.sub(r'\\d', ' ', str(document)) # Remove all the digits from text\n",
        "  document = re.sub(r'\\s+[A-z]\\s+', ' ', document) # Remove all single characters from text\n",
        "  document = re.sub(r'\\s+', ' ', document, flags=re.I) # Remove extra white space from text\n",
        "  document = document.lower() # Converting to Lowercase\n",
        "  return document"
      ],
      "metadata": {
        "id": "My5eNyDae8-O"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess(\"أحمد ضرب عمرو 1 2 3 4 5Ahmed hit Omar ...s   $##@    r\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "85vnhXGZi0el",
        "outputId": "1e146a9e-1513-4347-f036-552795395f44"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'أحمد ضرب عمرو 1 2 3 4 5ahmed hit omar r'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "processed_texts = [preprocess(doc) for doc in docs]\n",
        "processed_text = str(sum([processed_texts], [])[0])\n",
        "processed_text[:999]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "h7a5impOjDaz",
        "outputId": "6ffcf83a-4c47-4d08-a5f5-40a40ffbc2c3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'egypt arabic مصر miṣr mesˁr egyptian arabic pronunciation mɑsˤr officially the arab republic of egypt is transcontinental country spanning the northeast corner of africa and the sinai peninsula in the southwest corner of asia it is bordered by the mediterranean sea to the north the gaza strip of palestine and israel to the northeast the red sea to the east sudan to the south and libya to the west the gulf of aqaba in the northeast separates egypt from jordan and saudi arabia cairo is the capital and largest city of egypt while alexandria the second largest city is an important industrial and tourist hub at the mediterranean coast at approximately 100 million inhabitants egypt is the 14th most populated country in the world and the third most populated in africa egypt has one of the longest histories of any country tracing its heritage along the nile delta back to the 6th 4th millennia bce considered cradle of civilisation ancient egypt saw some of the earliest developments of writing '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Tokenization & Vectorization"
      ],
      "metadata": {
        "id": "PhAUsIUhj-3l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
        "tokens = tokenizer.tokenize(processed_text.lower())\n",
        "len(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02fEVacPMCf7",
        "outputId": "20f636a8-ddda-475e-d6b5-6babc217c79a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14038"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = keras.layers.TextVectorization(max_tokens=10000, output_sequence_length=200)\n",
        "vectorizer.adapt(np.array(processed_texts))\n"
      ],
      "metadata": {
        "id": "-PRAEA8V461a"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(vectorizer.get_vocabulary()), *vectorizer.get_vocabulary()[30:40]"
      ],
      "metadata": {
        "id": "N4TdbCjv5KnO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8b17405-dcd1-4946-e832-2cfae9a2eaf8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9713,\n",
              " 'this',\n",
              " 'also',\n",
              " 'mamluks',\n",
              " 'new',\n",
              " 'who',\n",
              " 'after',\n",
              " 'it',\n",
              " 'its',\n",
              " 'they',\n",
              " 'cairo')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "voc = vectorizer.get_vocabulary()\n",
        "word_index = dict(zip(voc, range(len(voc))))\n",
        "\n",
        "# word_index"
      ],
      "metadata": {
        "id": "9JCmfsQcFjKa"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_index['ali'], word_index['egypt'], word_index['islam']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmKCzN9w5hzD",
        "outputId": "55b61648-4396-489d-918f-0d42f99bc839"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(236, 8, 152)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Creating Data Sequences"
      ],
      "metadata": {
        "id": "ffFmhJwA9gva"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare the dataset of input to output pairs encoded as integers\n",
        "seq_length = 10\n",
        "n_words = len(tokens)\n",
        "\n",
        "dataX = []\n",
        "dataY = []\n",
        "\n",
        "for i in tqdm(range(0, n_words - seq_length, 1)):\n",
        "    seq_in = tokens[i:i + seq_length]\n",
        "    seq_out = tokens[i + seq_length]\n",
        "    dataX.append(seq_in)\n",
        "    dataY.append([seq_out])\n",
        "\n",
        "n_seqs = len(dataX)\n",
        "print(\"\\nTotal Sequences: \", n_seqs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_zeWMw79wAN",
        "outputId": "f3720896-71e1-46df-998c-1cc9f2b992f3"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14028/14028 [00:00<00:00, 247924.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Total Sequences:  14028\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# X = np.zeros((len(dataX), n_words, len(voc)), dtype=bool)  # for each sample, n input words and then a boolean for each possible next word\n",
        "# y = np.zeros((len(dataY), len(voc)), dtype=bool)  # for each sample a boolean for each possible next word"
      ],
      "metadata": {
        "id": "M4-JAy7LAeRS"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for i, words in enumerate(dataX):\n",
        "#     for j, word in enumerate(words):\n",
        "#         X[i, j, word_index[word]] = 1\n",
        "#     y[i, word_index[dataY[i][0]]] = 1"
      ],
      "metadata": {
        "id": "94tr3tcKPm6K"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_glove_file = \"glove.6B.300d.txt\"\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "    print(\"loading glove embeddings..\")\n",
        "    for line in tqdm(f.readlines()):\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))"
      ],
      "metadata": {
        "id": "9jIIhAOdN06q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b569ddcb-023f-466d-a70a-b2ca427112dc"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading glove embeddings..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 400000/400000 [00:24<00:00, 16351.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 400000 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_tokens = len(voc) + 2\n",
        "embedding_dim = 300\n",
        "hits = 0\n",
        "misses = 0\n",
        "\n",
        "# Prepare embedding matrix\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in embedding index will be all-zeros.\n",
        "        # This includes the representation for \"padding\" and \"OOV\"\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        hits += 1\n",
        "    else:\n",
        "        misses += 1\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))"
      ],
      "metadata": {
        "id": "KXhhZRfjN9i2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afbb173c-82fe-4cb5-c303-2e153150c4e0"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted 8913 words (800 misses)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.zeros((len(dataX), seq_length))\n",
        "Y = np.zeros((len(dataY), 300))\n",
        "\n",
        "for i, seq in enumerate(dataX):\n",
        "  for j, word in enumerate(seq):\n",
        "    X[i, j] = word_index[word]\n",
        "  Y[i] = embedding_matrix[word_index[dataY[i][0]]]"
      ],
      "metadata": {
        "id": "elrAB6GgOZpv"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tj1twrj5lVDb"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. RNN Model Building"
      ],
      "metadata": {
        "id": "z6jMxSPRj2ZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_layer = Embedding(\n",
        "    num_tokens,\n",
        "    embedding_dim,\n",
        "    trainable=False,\n",
        ")\n",
        "embedding_layer.build((1,))\n",
        "embedding_layer.set_weights([embedding_matrix])\n",
        "\n",
        "model = Sequential()\n",
        "model.add(embedding_layer)\n",
        "model.add(LSTM(128, input_shape=(n_words, len(voc)), return_sequences=True))\n",
        "model.add(LSTM(128))\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dense(300))\n",
        "# model.add(Activation(\"softmax\"))\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "2GvOh_vZEu0U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bde89dc-ddf6-4617-ac7a-4dc1252e540c"
      },
      "execution_count": 242,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_25\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_25 (Embedding)    (None, None, 300)         2914500   \n",
            "                                                                 \n",
            " lstm_49 (LSTM)              (None, None, 128)         219648    \n",
            "                                                                 \n",
            " lstm_50 (LSTM)              (None, 128)               131584    \n",
            "                                                                 \n",
            " dense_50 (Dense)            (None, 100)               12900     \n",
            "                                                                 \n",
            " dense_51 (Dense)            (None, 300)               30300     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3308932 (12.62 MB)\n",
            "Trainable params: 394432 (1.50 MB)\n",
            "Non-trainable params: 2914500 (11.12 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Model compiling & training"
      ],
      "metadata": {
        "id": "jhucFOtUGF0h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = Adam(learning_rate=0.01)\n",
        "model.compile(loss=\"cosine_similarity\", optimizer=optimizer, metrics=[\"accuracy\"])\n"
      ],
      "metadata": {
        "id": "urUw3M5xGS2U"
      },
      "execution_count": 243,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
        "\n",
        "history = model.fit(\n",
        "    X, Y,\n",
        "    validation_split=0.2,\n",
        "    batch_size=32,\n",
        "    epochs=10,\n",
        "    # callbacks=[early_stopping],\n",
        "    shuffle=True\n",
        ").history"
      ],
      "metadata": {
        "id": "TgnvO0cBR0DM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5180b179-3350-43b3-fd32-ab832880ad7c"
      },
      "execution_count": 244,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "351/351 [==============================] - 11s 15ms/step - loss: -0.4814 - accuracy: 0.3885 - val_loss: -0.4772 - val_accuracy: 0.4334\n",
            "Epoch 2/10\n",
            "351/351 [==============================] - 3s 10ms/step - loss: -0.4954 - accuracy: 0.3904 - val_loss: -0.4892 - val_accuracy: 0.4334\n",
            "Epoch 3/10\n",
            "351/351 [==============================] - 3s 7ms/step - loss: -0.5043 - accuracy: 0.3916 - val_loss: -0.4902 - val_accuracy: 0.4334\n",
            "Epoch 4/10\n",
            "351/351 [==============================] - 2s 7ms/step - loss: -0.5107 - accuracy: 0.3917 - val_loss: -0.4936 - val_accuracy: 0.4277\n",
            "Epoch 5/10\n",
            "351/351 [==============================] - 3s 7ms/step - loss: -0.5172 - accuracy: 0.3912 - val_loss: -0.4948 - val_accuracy: 0.4316\n",
            "Epoch 6/10\n",
            "351/351 [==============================] - 3s 8ms/step - loss: -0.5230 - accuracy: 0.3899 - val_loss: -0.4891 - val_accuracy: 0.4152\n",
            "Epoch 7/10\n",
            "351/351 [==============================] - 4s 11ms/step - loss: -0.5281 - accuracy: 0.3931 - val_loss: -0.4968 - val_accuracy: 0.4244\n",
            "Epoch 8/10\n",
            "351/351 [==============================] - 4s 11ms/step - loss: -0.5330 - accuracy: 0.3903 - val_loss: -0.4936 - val_accuracy: 0.4198\n",
            "Epoch 9/10\n",
            "351/351 [==============================] - 4s 12ms/step - loss: -0.5394 - accuracy: 0.3977 - val_loss: -0.4953 - val_accuracy: 0.4220\n",
            "Epoch 10/10\n",
            "351/351 [==============================] - 4s 11ms/step - loss: -0.5450 - accuracy: 0.3994 - val_loss: -0.4920 - val_accuracy: 0.3949\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_nearest_word_vec(\n",
        "    vec,\n",
        "    embedding_matrix = embedding_matrix,\n",
        "    voc = voc\n",
        "    ):\n",
        "  sims = cosine_similarity(embedding_matrix, vec)\n",
        "  nearest_word_idx = np.argmax(sims)\n",
        "  return nearest_word_idx, voc[nearest_word_idx]\n"
      ],
      "metadata": {
        "id": "Tv1_Qb6ElGO9"
      },
      "execution_count": 245,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_next_word(\n",
        "    sent,\n",
        "    model = model\n",
        "):\n",
        "  sent = preprocess(sent)\n",
        "  tokened_sent = tokenizer.tokenize(sent)\n",
        "  vec_text = np.array([word_index[word] for word in tokened_sent]).reshape(1, -1)\n",
        "  if vec_text.shape[1] == seq_length:\n",
        "    diff = seq_length - vec_text.shape[1]\n",
        "    vec_text = np.concatenate([[0]*diff, vec_text.reshape(-1)]).reshape(1, -1)\n",
        "  predicted_next_word = model.predict(vec_text, verbose=0)\n",
        "  return get_nearest_word_vec(predicted_next_word)"
      ],
      "metadata": {
        "id": "yAoo88GLvtcR"
      },
      "execution_count": 246,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(\n",
        "    sent,\n",
        "    max_tokens = 100\n",
        "):\n",
        "  acc_text = sent\n",
        "  for x in range(max_tokens):\n",
        "    _, next_word = get_next_word(\" \".join(acc_text.split()[-30:]))\n",
        "    acc_text += \" \"+next_word\n",
        "  return acc_text"
      ],
      "metadata": {
        "id": "hzPQdREa0lfn"
      },
      "execution_count": 247,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"egypt has one of the longest histories of any country tracing its heritage along the nile delta back to the 6th 4th millennia bce considered cradle of civilisation ancient egypt\"\n",
        "\n",
        "generate(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "I52N0rjiwNu6",
        "outputId": "77d771c7-89af-4e80-b456-5aef65789779"
      },
      "execution_count": 254,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'egypt has one of the longest histories of any country tracing its heritage along the nile delta back to the 6th 4th millennia bce considered cradle of civilisation ancient egypt when of the earliest country in east early century egypt since the century egypt in the east area the century dynasty area the century egypt was but to the but of the century ottoman empire in average but to to but have since the in predominantly muslims in egypt and the but since egypt as part this its country in the one years which as country which established which established its development after increased agriculture to well and gas these especially areas and sector result because was increase in increase in increase in increase in 2007 month month according that'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 254
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# egypt has one of the longest histories of any country tracing its heritage along the nile delta back to the 6th 4th millennia bce\n",
        "# considered cradle of civilisation ancient egypt when of the earliest country in east early century egypt since the century egypt in\n",
        "# the east area the century dynasty area the century egypt was but to the but of the century ottoman empire in average but to to but\n",
        "# have since the in predominantly muslims in egypt and the but since egypt as part this its country in the one years which as country\n",
        "# which established which established its development after increased agriculture to well and gas these especially areas and sector\n",
        "# result because was increase in increase in increase in increase in 2007 month month according that"
      ],
      "metadata": {
        "id": "CzCP5vuBtocr"
      },
      "execution_count": 253,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"RNN_Model.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gG5AGcW5BNlb",
        "outputId": "da1dec76-b551-4f7c-cc2c-6244fd37099e"
      },
      "execution_count": 249,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2L2Aeu1tF1Bp"
      },
      "execution_count": 230,
      "outputs": []
    }
  ]
}